---
title: "Class 08: Breast Cancer Analysis Project"
author: 'Ethan Ashley (PID: A15939817)'
date: "2025-10-24"
output: 
  github_document:
    toc: true
    toc_depth: 2
---

# Background

The goal of this mini-project is for you to explore a complete analysis using the unsupervised learning techniques covered in class. We will extend what we've learned by combining PCA as a preprocessing step to clustering using data that consist of measurements of cell nuclei of human breast masses. This expands on our RNA-Seq analysis from last day.

The data itself comes from the Wisconsin Breast Cancer Diagnostic Data Set first reported by K. P. Benne and O. L. Mangasarian: “Robust Linear Programming Discrimination of Two Linearly Inseparable Sets”.

Values in this data set describe characteristics of the cell nuclei present in digitized images of a fine needle aspiration (FNA) of a breast mass.


# Data Import

Importing the Wisconsin Cancer data set. Make sure not to include the patient IDs or the pathologists diagnoses in the data that we analyze below.

```{r}
fna.data <- "WisconsinCancer.csv"

wisc.df <- read.csv(fna.data, row.names = 1)

# We can use -1 here to remove the first column containing the pathologist classifications
diagnosis <- wisc.df$diagnosis
wisc.data <- wisc.df[,-1]
wisc.data <- wisc.data[,-31]
```


# Exploratory Data Analysis

**Q1. How many observations are in this dataset?**

There are 569 observations in the dataset.

```{r}
nrow(wisc.data)
```
**Q2. How many of the observations have a malignant diagnosis?**

There are 212 twelve observations with a malignant diagnosis.

```{r}
sum(diagnosis == "M")

table(wisc.df$diagnosis) #alternative method shown in class
```

**Q3. How many variables/features in the data are suffixed with _mean?**

There are 10 variables in the data that are suffixed with _mean.

```{r}
length(grep("_mean", colnames(wisc.data)))
```

# Principal Component Analysis

The main function in base R for PCA is `prcomp()`. It has optional inputs scale and center. 

In general we want to scale and center our data prior to PCA to ensure that each feature contributes equally to the analysis, preventing variables with larger scales from dominating the principal components.

```{r}
# Check column means and standard deviations
colMeans(wisc.data)

apply(wisc.data,2,sd)
```

Scaling and centering is advisable with this dataset.

```{r}
# Perform PCA on wisc.data by completing the following code
wisc.pr <- prcomp(wisc.data, scale. = TRUE)
```

**Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?**

44.27% of the original variance is accounted for by PC1.

**Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?**

3 PCs are required to describe at least 70% of the original variance in the dataset.

**Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?**

7 PCs are required to describe at least 90% of the original variance in the dataset.

```{r}
summary(wisc.pr)
```


## Interpreting PCA results

**Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?**

This plot is very messy and difficult to read. It is not possible to tell which points come from which diagnosis group.


```{r}
biplot(wisc.pr)
```

Let's make our main results figure - the "PC plot" or "score plot".

```{r}
library(ggplot2)

ggplot(wisc.pr$x) + aes(PC1, PC2, color = diagnosis) + geom_point()
```

**Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?**

There is still good clustering of the benign and malignant tumors. Since PC3 explains less of the variance than PC2, it doesn't do quite as good of a job of separating the two groups.


```{r}
ggplot(wisc.pr$x) + aes(PC1, PC3, color = diagnosis) + geom_point()
```

## Variance Explained

```{r}
# Calculate variance of each component
pr.var <- wisc.pr$sdev^2
head(pr.var)
```

```{r}
# Variance explained by each principal component: pve
pve <- pr.var / sum(pr.var)

# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```

**Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean? This tells us how much this original feature contributes to the first PC.**

The component of the loading vector for PC1 for the feature concave.points_mean is -0.26085376.


```{r}
wisc.pr$rotation[,1]
```
# Hierarchical Clustering

```{r}
# Scale the wisc.data data using the "scale()" function
data.scaled <- scale(wisc.data)
```


```{r}
data.dist <- dist(data.scaled)
```


```{r}
wisc.hclust <- hclust(data.dist, method="complete")
```

**Q10. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?**

A height of 19 appears to lead to 4 clusters.

```{r}
plot(wisc.hclust)
abline(h=19, col="red", lty=2)
```

## Using Different Methods

**Q12. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning.**

The method ward.D2 produces my favorite results for the data.dist. It is the easiest of the various options to tell where branches of clusters form.

```{r}
wisc.hclust <- hclust(data.dist, method="ward.D2")

plot(wisc.hclust)
```

# Combining Methods

```{r}
wisc.pr.hclust <- hclust(dist(wisc.pr$x[,1:7]), method="ward.D2")

plot(wisc.pr.hclust)
```

```{r}
grps <- cutree(wisc.pr.hclust, h=70)
table(grps)

table(grps, diagnosis)

plot(wisc.pr$x[,1:2], col=grps)

```
There are 188 true positives and 28 false positives in the malignant group.
There are 329 true positives and 24 false positives in the benign group.

```{r}
g <- as.factor(grps)
levels(g)

g <- relevel(g,2)
levels(g)

# Plot using our re-ordered factor 
plot(wisc.pr$x[,1:2], col=g)
```

```{r}
library(rgl)
plot3d(wisc.pr$x[,1:3], xlab="PC 1", ylab="PC 2", zlab="PC 3", cex=1.5, size=1, type="s", col=grps)
```

**Q13. How well does the newly created model with four clusters separate out the two diagnoses?**

In general, the newly created model is fairly accurate. However, in a clinical setting, the false negatives and positives would not be desirable.

```{r}
## Use the distance along the first 7 PCs for clustering i.e. wisc.pr$x[, 1:7]
wisc.pr.hclust <- hclust(dist(wisc.pr$x[, 1:7]), method="ward.D2")

wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k=2)

table(wisc.pr.hclust.clusters, diagnosis)
```
**Q14. How well do the hierarchical clustering models you created in previous sections (i.e. before PCA) do in terms of separating the diagnoses? Again, use the table() function to compare the output of each model (wisc.km$cluster and wisc.hclust.clusters) with the vector containing the actual diagnoses.**

These other methods appear to do a fairly good job of classifying the data.

```{r}
wisc.km <- kmeans(wisc.data, 2)

table(wisc.km$cluster, diagnosis)
```

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k=2)

table(wisc.hclust.clusters, diagnosis)
```


# Prediction

```{r}
#url <- "new_samples.csv"
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)

plot(wisc.pr$x[,1:2], col=g)
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], c(1,2), col="white")
```

**Q16. Which of these new patients should we prioritize for follow up based on your results?**

I think we should prioritize patient 2 as they fall into the malignant tumor group.
